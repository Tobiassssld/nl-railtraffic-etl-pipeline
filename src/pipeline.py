# src/pipeline.py

import sys
import json
from pathlib import Path
from datetime import datetime
import pandas as pd

# å¯¼å…¥æˆ‘ä»¬ä¹‹å‰å†™çš„æ¨¡å—
from ingestion.api_client import NSAPIClient
from storage.database import Database
from transformation.cleaners import DisruptionCleaner
from config import setup_logging


class ETLPipeline:
    """
    å®Œæ•´çš„ETLæµç¨‹
    Extractï¼ˆæå–ï¼‰ â†’ Transformï¼ˆè½¬æ¢ï¼‰ â†’ Loadï¼ˆåŠ è½½ï¼‰
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–pipelineç»„ä»¶
        """
        self.logger = setup_logging()
        self.logger.info("=" * 60)
        self.logger.info("ğŸš€ NS Rail Traffic ETL Pipeline å¯åŠ¨")
        self.logger.info("=" * 60)
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        try:
            self.api_client = NSAPIClient()
            self.database = Database()
            self.cleaner = DisruptionCleaner()
            self.logger.info("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def run(self):
        """
        æ‰§è¡Œå®Œæ•´çš„ETLæµç¨‹
        """
        try:
            # ===== æ­¥éª¤1: Extractï¼ˆæå–ï¼‰=====
            self.logger.info("\nğŸ“¥ æ­¥éª¤1: ä»NS APIæå–æ•°æ®...")
            raw_data = self._extract()
            
            if not raw_data:
                self.logger.warning("âš ï¸  æ²¡æœ‰è·å–åˆ°æ•°æ®ï¼Œpipelineç»ˆæ­¢")
                return
            
            # ===== æ­¥éª¤2: Transformï¼ˆè½¬æ¢ï¼‰=====
            self.logger.info("\nğŸ”„ æ­¥éª¤2: æ¸…æ´—å’Œè½¬æ¢æ•°æ®...")
            cleaned_data = self._transform(raw_data)
            
            if cleaned_data.empty:
                self.logger.warning("âš ï¸  æ¸…æ´—åæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œpipelineç»ˆæ­¢")
                return
            
            # ===== æ­¥éª¤3: Loadï¼ˆåŠ è½½ï¼‰=====
            self.logger.info("\nğŸ’¾ æ­¥éª¤3: åŠ è½½æ•°æ®åˆ°æ•°æ®åº“...")
            self._load(raw_data, cleaned_data)
            
            # ===== æ­¥éª¤4: ç”ŸæˆæŠ¥å‘Š =====
            self.logger.info("\nğŸ“Š æ­¥éª¤4: ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
            self._generate_report()
            
            self.logger.info("\n" + "=" * 60)
            self.logger.info("âœ… Pipelineæ‰§è¡ŒæˆåŠŸï¼")
            self.logger.info("=" * 60)
            
        except Exception as e:
            self.logger.error(f"\nâŒ Pipelineæ‰§è¡Œå¤±è´¥: {e}")
            self.logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š")
            raise
    
    def _extract(self):
        """
        æ­¥éª¤1: ä»APIæå–æ•°æ®
        
        è¿”å›: list - åŸå§‹æ•°æ®
        """
        try:
            disruptions = self.api_client.fetch_disruptions()
            self.logger.info(f"   è·å–åˆ° {len(disruptions)} æ¡å»¶è¯¯è®°å½•")
            return disruptions
        except Exception as e:
            self.logger.error(f"   æ•°æ®æå–å¤±è´¥: {e}")
            return []
    
    def _transform(self, raw_data):
        """
        æ­¥éª¤2: æ¸…æ´—æ•°æ®
        
        å‚æ•°:
            raw_data: list - åŸå§‹æ•°æ®
        
        è¿”å›:
            pd.DataFrame - æ¸…æ´—åçš„æ•°æ®
        """
        try:
            cleaned_df = self.cleaner.clean(raw_data)
            self.logger.info(f"   æ¸…æ´—åä¿ç•™ {len(cleaned_df)} æ¡æœ‰æ•ˆè®°å½•")
            
            # ä¿å­˜æ¸…æ´—åçš„æ•°æ®åˆ°CSVï¼ˆç”¨äºæ£€æŸ¥ï¼‰
            output_path = Path("data/processed") / f"cleaned_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            output_path.parent.mkdir(parents=True, exist_ok=True)
            cleaned_df.to_csv(output_path, index=False, encoding='utf-8-sig')
            self.logger.info(f"   æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
            
            return cleaned_df
            
        except Exception as e:
            self.logger.error(f"   æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _load(self, raw_data, cleaned_data):
        """
        æ­¥éª¤3: åŠ è½½æ•°æ®åˆ°æ•°æ®åº“
        
        å‚æ•°:
            raw_data: list - åŸå§‹æ•°æ®
            cleaned_data: pd.DataFrame - æ¸…æ´—åçš„æ•°æ®
        """
        try:
            # 3.1 ä¿å­˜åŸå§‹æ•°æ®åˆ° raw_disruptions è¡¨
            self.logger.info("   3.1 ä¿å­˜åŸå§‹æ•°æ®...")
            self._save_raw_data(raw_data)
            
            # 3.2 ä¿å­˜æ¸…æ´—åçš„æ•°æ®åˆ° disruptions è¡¨
            self.logger.info("   3.2 ä¿å­˜æ¸…æ´—åçš„æ•°æ®...")
            self._save_cleaned_data(cleaned_data)
            
            self.logger.info("   âœ… æ•°æ®åŠ è½½å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"   æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _save_raw_data(self, raw_data):
        """
        ä¿å­˜åŸå§‹JSONåˆ°æ•°æ®åº“
        
        ä½¿ç”¨ INSERT OR IGNORE é¿å…é‡å¤
        """
        inserted = 0
        skipped = 0
        
        for item in raw_data:
            try:
                disruption_id = item.get('id')
                if not disruption_id:
                    continue
                
                # è½¬æˆJSONå­—ç¬¦ä¸²
                raw_json = json.dumps(item, ensure_ascii=False)
                
                # æ’å…¥æ•°æ®åº“ï¼ˆå¦‚æœå·²å­˜åœ¨åˆ™å¿½ç•¥ï¼‰
                self.database.cursor.execute("""
                    INSERT OR IGNORE INTO raw_disruptions 
                    (disruption_id, raw_json) 
                    VALUES (?, ?)
                """, (disruption_id, raw_json))
                
                # æ£€æŸ¥æ˜¯å¦çœŸçš„æ’å…¥äº†ï¼ˆrowcount=1è¡¨ç¤ºæ’å…¥æˆåŠŸï¼‰
                if self.database.cursor.rowcount > 0:
                    inserted += 1
                else:
                    skipped += 1
                    
            except Exception as e:
                self.logger.warning(f"      ä¿å­˜åŸå§‹æ•°æ®å¤±è´¥ (ID: {disruption_id}): {e}")
        
        self.database.conn.commit()
        self.logger.info(f"      æ’å…¥ {inserted} æ¡ï¼Œè·³è¿‡ {skipped} æ¡é‡å¤æ•°æ®")
    
    def _save_cleaned_data(self, df):
        """
        ä¿å­˜æ¸…æ´—åçš„æ•°æ®åˆ°æ•°æ®åº“
        
        ä½¿ç”¨ UPSERT é€»è¾‘ï¼š
        - å¦‚æœdisruption_idå·²å­˜åœ¨ â†’ æ›´æ–°
        - å¦‚æœä¸å­˜åœ¨ â†’ æ’å…¥
        """
        # ç¡®ä¿æ—¶é—´åˆ—æ ¼å¼æ­£ç¡®
        datetime_columns = ['start_time', 'end_time', 'created_at', 'updated_at']
        for col in datetime_columns:
            if col in df.columns:
                # è½¬æˆå­—ç¬¦ä¸²æ ¼å¼ï¼ˆSQLiteå…¼å®¹ï¼‰
                df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # ä½¿ç”¨pandasçš„to_sqlæ–¹æ³•
        # if_exists='append': å¦‚æœè¡¨å­˜åœ¨åˆ™è¿½åŠ 
        # ä½†è¿™ä¼šå¯¼è‡´é‡å¤ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨æ‰‹åŠ¨çš„UPSERT
        
        inserted = 0
        updated = 0
        
        for idx, row in df.iterrows():
            try:
                # æ£€æŸ¥è®°å½•æ˜¯å¦å·²å­˜åœ¨
                self.database.cursor.execute(
                    "SELECT id FROM disruptions WHERE disruption_id = ?",
                    (row['disruption_id'],)
                )
                exists = self.database.cursor.fetchone()
                
                if exists:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    self.database.cursor.execute("""
                        UPDATE disruptions SET
                            type = ?,
                            title = ?,
                            description = ?,
                            start_time = ?,
                            end_time = ?,
                            duration_minutes = ?,
                            impact_level = ?,
                            affected_stations = ?,
                            updated_at = ?
                        WHERE disruption_id = ?
                    """, (
                        row.get('type'),
                        row.get('title'),
                        row.get('description'),
                        row.get('start_time'),
                        row.get('end_time'),
                        row.get('duration_minutes') if pd.notna(row.get('duration_minutes')) else None,
                        row.get('impact_level'),
                        row.get('affected_stations'),
                        row.get('updated_at'),
                        row['disruption_id']
                    ))
                    updated += 1
                else:
                    # æ’å…¥æ–°è®°å½•
                    self.database.cursor.execute("""
                        INSERT INTO disruptions (
                            disruption_id, type, title, description,
                            start_time, end_time, duration_minutes,
                            impact_level, affected_stations,
                            is_resolved, created_at, updated_at
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        row['disruption_id'],
                        row.get('type'),
                        row.get('title'),
                        row.get('description'),
                        row.get('start_time'),
                        row.get('end_time'),
                        row.get('duration_minutes') if pd.notna(row.get('duration_minutes')) else None,
                        row.get('impact_level'),
                        row.get('affected_stations'),
                        row.get('is_resolved', 0),
                        row.get('created_at'),
                        row.get('updated_at')
                    ))
                    inserted += 1
                    
            except Exception as e:
                self.logger.warning(f"      ä¿å­˜è®°å½•å¤±è´¥ (ID: {row['disruption_id']}): {e}")
        
        self.database.conn.commit()
        self.logger.info(f"      æ’å…¥ {inserted} æ¡ï¼Œæ›´æ–° {updated} æ¡")
    
    def _generate_report(self):
        """
        ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        """
        try:
            # æŸ¥è¯¢å½“å‰æ•°æ®åº“ç»Ÿè®¡
            self.database.cursor.execute("""
                SELECT 
                    COUNT(*) as total,
                    SUM(CASE WHEN type = 'disruption' THEN 1 ELSE 0 END) as disruptions,
                    SUM(CASE WHEN type = 'maintenance' THEN 1 ELSE 0 END) as maintenance,
                    SUM(CASE WHEN type = 'calamity' THEN 1 ELSE 0 END) as calamity,
                    AVG(duration_minutes) as avg_duration,
                    MAX(impact_level) as max_impact
                FROM disruptions
                WHERE DATE(created_at) = DATE('now')
            """)
            
            stats = self.database.cursor.fetchone()
            
            self.logger.info("\n   ğŸ“ˆ ä»Šæ—¥æ•°æ®ç»Ÿè®¡ï¼š")
            self.logger.info(f"      æ€»è®°å½•æ•°: {stats[0]}")
            self.logger.info(f"      å»¶è¯¯(disruption): {stats[1]}")
            self.logger.info(f"      ç»´æŠ¤(maintenance): {stats[2]}")
            self.logger.info(f"      ç¾éš¾(calamity): {stats[3]}")
            self.logger.info(f"      å¹³å‡æŒç»­æ—¶é—´: {stats[4]:.1f} åˆ†é’Ÿ" if stats[4] else "      å¹³å‡æŒç»­æ—¶é—´: N/A")
            self.logger.info(f"      æœ€é«˜å½±å“çº§åˆ«: {stats[5]}")
            
        except Exception as e:
            self.logger.warning(f"   ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")


# ===== ä¸»ç¨‹åºå…¥å£ =====
def main():
    """
    ä¸»å‡½æ•°
    """
    try:
        pipeline = ETLPipeline()
        pipeline.run()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()